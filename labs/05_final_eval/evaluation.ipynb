{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561f2017",
   "metadata": {},
   "source": [
    "# Lab 5 · Final Evaluation\n",
    "\n",
    "Evaluate the final CSE472 Blanket submissions: download predictions, validate them against the hidden ground truth, compute RMSE/Jaccard/combined score, and export figures to `fig/`.\n",
    "\n",
    "- pulls submission links from `team.jsonl`\n",
    "- caches CSVs in `data/`\n",
    "- validates schema and array lengths\n",
    "- reports a leaderboard plus diagnostics plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aabccc",
   "metadata": {},
   "source": [
    "## Team roster & submission links\n",
    "\n",
    "The links below are read from `team.jsonl` and used for downloading each submission.\n",
    "\n",
    "| Group | Members | Repo | Submission File |\n",
    "|-------|---------|------|-----------------|\n",
    "| 1 | Sameera, Tanmayi | [sameerashahh/CSE472-blanket-challenge](https://github.com/sameerashahh/CSE472-blanket-challenge/tree/project2_implementation) | [`final_project_implementation/submission.csv`](https://raw.githubusercontent.com/sameerashahh/CSE472-blanket-challenge/486287884d7099c273a570a16aec4d387f60320e/final_project_implementation/submission.csv) |\n",
    "| 7 | Dhruv, Sahajpreet | [dhruvb26/CSE472-blanket-challenge](https://github.com/dhruvb26/CSE472-blanket-challenge/tree/main) | [`solution/runs/20251124_194116/submission.csv`](https://raw.githubusercontent.com/dhruvb26/CSE472-blanket-challenge/b8c8ca86d3f6d0aa97ba79cd1f6d73ca57018c50/solution/runs/20251124_194116/submission.csv) |\n",
    "| 8 | Fredo, Anton | [saan-volta/CSE472-blanket-challenge-submission](https://github.com/saan-volta/CSE472-blanket-challenge-submission/tree/main) | [`submission/submission.csv`](https://raw.githubusercontent.com/saan-volta/CSE472-blanket-challenge-submission/942d6b74e42ec51330fdfc8505c992db356819cf/submission/submission.csv) |\n",
    "| 10 | Ang, Muhammed | [muhammedhunaid/CSE472-blanket-challenge](https://github.com/muhammedhunaid/CSE472-blanket-challenge/tree/muhammed/final-submission) | [`submission.csv`](https://raw.githubusercontent.com/muhammedhunaid/CSE472-blanket-challenge/refs/heads/muhammed/final-submission/submission.csv) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e888c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Figures are written to `fig/`, cached submissions live in `data/`, and the hidden ground truth is loaded from the sibling `blanket` repo. Run the notebook top-to-bottom after ensuring the `blanket` environment is active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ensure local blanket package is importable\n",
    "LAB_ROOT = Path().resolve()\n",
    "PROJECT_SRC = LAB_ROOT.parents[2] / \"blanket\" / \"src\"\n",
    "sys.path.append(str(PROJECT_SRC))\n",
    "from blanket.metrics import jaccard_score, rmse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 8)\n",
    "\n",
    "DATA_DIR = LAB_ROOT / \"data\"\n",
    "FIG_DIR = LAB_ROOT / \"fig\"\n",
    "GROUND_TRUTH_DIR = (\n",
    "    LAB_ROOT.parents[2] / \"blanket\" / \"data\" / \"datasets\" / \"final-ground-truth\"\n",
    ")\n",
    "\n",
    "for path in (DATA_DIR, FIG_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Lab root: {LAB_ROOT}\")\n",
    "print(f\"Ground truth dir: {GROUND_TRUTH_DIR}\")\n",
    "print(f\"Figures will be saved to: {FIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52998cda",
   "metadata": {},
   "source": [
    "## Load ground-truth dataset\n",
    "\n",
    "We keep the provided HF splits to double-check IDs, then convert the held-out test split to a Pandas frame with `y_test` and the Markov blanket mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "develop = load_dataset(\n",
    "    \"CSE472-blanket-challenge/final-dataset\", \"develop\", split=\"train\"\n",
    ")\n",
    "submit = load_dataset(\"CSE472-blanket-challenge/final-dataset\", \"submit\", split=\"train\")\n",
    "ground_truth_data = load_from_disk(GROUND_TRUTH_DIR)\n",
    "\n",
    "assert set(develop[\"data_id\"]) == set(ground_truth_data[\"train\"][\"data_id\"])\n",
    "assert set(submit[\"data_id\"]) == set(ground_truth_data[\"test\"][\"data_id\"])\n",
    "\n",
    "ground_truth = (\n",
    "    ground_truth_data[\"test\"]\n",
    "    .select_columns([\"data_id\", \"y_test\", \"feature_mask\"])\n",
    "    .rename_column(\"feature_mask\", \"markov_blanket\")\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "ground_truth.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8a51c",
   "metadata": {},
   "source": [
    "## Download & cache submissions\n",
    "\n",
    "URLs come from `team.jsonl`. We skip downloads when the file already exists to keep runs fast and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_submission(url: str, save_path: Path, overwrite: bool = False) -> Path:\n",
    "    if save_path.exists() and not overwrite:\n",
    "        return save_path\n",
    "\n",
    "    response = httpx.get(url, timeout=30.0)\n",
    "    response.raise_for_status()\n",
    "    save_path.write_bytes(response.content)\n",
    "    return save_path\n",
    "\n",
    "\n",
    "teams = load_dataset(\"json\", data_files=str(LAB_ROOT / \"team.jsonl\"), split=\"train\")\n",
    "\n",
    "for team in teams:\n",
    "    dest = DATA_DIR / f\"team_{team['group']}_submission.csv\"\n",
    "    download_submission(team[\"submission_url\"], dest)\n",
    "\n",
    "sorted(DATA_DIR.iterdir())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6ecc1",
   "metadata": {},
   "source": [
    "## Load submissions into data frames\n",
    "\n",
    "Parse the list-like columns into NumPy arrays and drop stray index columns if present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d821b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(value, dtype=float) -> np.ndarray:\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value.astype(dtype)\n",
    "    if isinstance(value, list):\n",
    "        return np.asarray(value, dtype=dtype)\n",
    "\n",
    "    text = str(value).strip()\n",
    "    if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        text = text[1:-1]\n",
    "    return np.fromstring(text, sep=\",\", dtype=dtype)\n",
    "\n",
    "\n",
    "submissions: dict[int, pd.DataFrame] = {}\n",
    "\n",
    "for team in teams:\n",
    "    group = team[\"group\"]\n",
    "    df = pd.read_csv(DATA_DIR / f\"team_{group}_submission.csv\")\n",
    "\n",
    "    unnamed_cols = [c for c in df.columns if c.startswith(\"Unnamed:\")]\n",
    "    if unnamed_cols:\n",
    "        df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "    df[\"y_pred\"] = df[\"y_pred\"].apply(lambda x: to_array(x, dtype=float))\n",
    "    df[\"markov_blanket_pred\"] = df[\"markov_blanket_pred\"].apply(\n",
    "        lambda x: to_array(x, dtype=int)\n",
    "    )\n",
    "\n",
    "    submissions[group] = df\n",
    "\n",
    "submissions[next(iter(submissions))].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86797a1c",
   "metadata": {},
   "source": [
    "## Validate submission format\n",
    "\n",
    "Quick schema and shape checks to surface any mismatches before scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(\n",
    "    submission: pd.DataFrame, ground_truth: pd.DataFrame, *, name: str\n",
    ") -> None:\n",
    "    expected_columns = {\"data_id\", \"y_pred\", \"markov_blanket_pred\"}\n",
    "    assert set(submission.columns) == expected_columns, (\n",
    "        f\"{name}: columns {submission.columns} do not match {expected_columns}\"\n",
    "    )\n",
    "\n",
    "    assert set(submission[\"data_id\"]) == set(ground_truth[\"data_id\"]), (\n",
    "        f\"{name}: data_id values differ from ground truth\"\n",
    "    )\n",
    "\n",
    "    assert submission.shape[0] == ground_truth.shape[0], (\n",
    "        f\"{name}: row count {submission.shape[0]} != ground truth {ground_truth.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    merged = submission.merge(ground_truth, on=\"data_id\", how=\"inner\")\n",
    "    for _, row in merged.iterrows():\n",
    "        assert len(row[\"y_pred\"]) == len(row[\"y_test\"]), (\n",
    "            f\"{name}: y_pred length {len(row['y_pred'])} does not match y_test length {len(row['y_test'])} for data_id {row['data_id']}\"\n",
    "        )\n",
    "        assert len(row[\"markov_blanket_pred\"]) == len(row[\"markov_blanket\"]), (\n",
    "            f\"{name}: markov_blanket_pred length {len(row['markov_blanket_pred'])} does not match markov_blanket length {len(row['markov_blanket'])} for data_id {row['data_id']}\"\n",
    "        )\n",
    "\n",
    "    print(f\"✅ {name}: schema and lengths look good\")\n",
    "\n",
    "\n",
    "for group, df in submissions.items():\n",
    "    validate_dataset(df, ground_truth, name=f\"Team {group}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115f986",
   "metadata": {},
   "source": [
    "## Evaluate metrics\n",
    "\n",
    "Score each task with RMSE and Jaccard, then combine them via `score = rmse × (1 - jaccard)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eed3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_task(\n",
    "    y_query_true: np.ndarray,\n",
    "    y_query_pred: np.ndarray,\n",
    "    mb_true: np.ndarray,\n",
    "    mb_pred: np.ndarray,\n",
    ") -> dict:\n",
    "    rmse_val = rmse(y_query_true, y_query_pred)\n",
    "    jaccard_val = jaccard_score(mb_true, mb_pred)\n",
    "    score_val = rmse_val * (1.0 - jaccard_val)\n",
    "    return {\"rmse\": rmse_val, \"jaccard\": jaccard_val, \"score\": score_val}\n",
    "\n",
    "\n",
    "def evaluate_submission(\n",
    "    submission: pd.DataFrame, ground_truth: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    merged = submission.merge(ground_truth, on=\"data_id\", how=\"inner\")\n",
    "    results = []\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        eval_result = evaluate_single_task(\n",
    "            y_query_true=row[\"y_test\"],\n",
    "            y_query_pred=row[\"y_pred\"],\n",
    "            mb_true=row[\"markov_blanket\"],\n",
    "            mb_pred=row[\"markov_blanket_pred\"],\n",
    "        )\n",
    "        eval_result[\"data_id\"] = row[\"data_id\"]\n",
    "        results.append(eval_result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "eval_results = {\n",
    "    group: evaluate_submission(df, ground_truth) for group, df in submissions.items()\n",
    "}\n",
    "eval_results[next(iter(eval_results))].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985e298",
   "metadata": {},
   "source": [
    "## Aggregate leaderboard\n",
    "\n",
    "Compute per-team means and standard deviations for each metric, then sort by the final score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08789048",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info = teams.to_pandas()\n",
    "team_info[\"members\"] = team_info[\"members\"].apply(lambda x: \", \".join(x))\n",
    "\n",
    "agg_rows = []\n",
    "for group, df in eval_results.items():\n",
    "    agg_rows.append(\n",
    "        {\n",
    "            \"group\": group,\n",
    "            \"rmse_mean\": df[\"rmse\"].mean(),\n",
    "            \"rmse_std\": df[\"rmse\"].std(),\n",
    "            \"jaccard_mean\": df[\"jaccard\"].mean(),\n",
    "            \"jaccard_std\": df[\"jaccard\"].std(),\n",
    "            \"score_mean\": df[\"score\"].mean(),\n",
    "            \"score_std\": df[\"score\"].std(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "agg_results_df = (\n",
    "    pd.DataFrame(agg_rows)\n",
    "    .merge(team_info[[\"group\", \"members\"]], on=\"group\", how=\"left\")\n",
    "    .sort_values(by=\"score_mean\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "agg_results_df.insert(0, \"rank\", range(1, len(agg_results_df) + 1))\n",
    "agg_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5e857",
   "metadata": {},
   "source": [
    "## Additional diagnostics\n",
    "\n",
    "Build a long-form frame for task-level analysis and capture quick stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11174a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.concat(\n",
    "    [df.assign(group=group) for group, df in eval_results.items()],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "corr_rmse_jaccard = score_df[\"rmse\"].corr(score_df[\"jaccard\"])\n",
    "print(f\"RMSE vs Jaccard correlation across tasks: {corr_rmse_jaccard:.3f}\")\n",
    "score_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9d9c9",
   "metadata": {},
   "source": [
    "## Metric summary plot\n",
    "\n",
    "Bar plots of team-level means with standard deviation error bars. Figures are saved under `fig/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = agg_results_df.copy()\n",
    "plot_df[\"team_label\"] = plot_df[\"group\"].apply(lambda x: f\"Team {x}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "metrics = [\n",
    "    (\"rmse_mean\", \"rmse_std\", \"RMSE\", \"Blues\"),\n",
    "    (\"jaccard_mean\", \"jaccard_std\", \"Jaccard\", \"Greens\"),\n",
    "    (\"score_mean\", \"score_std\", \"Score = RMSE × (1 - Jaccard)\", \"Oranges\"),\n",
    "]\n",
    "\n",
    "for ax, (mean_col, std_col, title, palette) in zip(axes, metrics):\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        x=\"team_label\",\n",
    "        y=mean_col,\n",
    "        yerr=plot_df[std_col],\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.3f\", padding=2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(title)\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Team-level metrics\", y=1.04, fontsize=16, fontweight=\"bold\")\n",
    "fig.savefig(FIG_DIR / \"team_metric_summary.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e3dea",
   "metadata": {},
   "source": [
    "## Trade-off & score distributions\n",
    "\n",
    "- RMSE vs. Jaccard scatter shows the regression/feature-selection trade-off.\n",
    "- Box plot shows score spread per team across tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_order = agg_results_df[\"group\"].tolist()\n",
    "\n",
    "# RMSE vs Jaccard scatter\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.scatterplot(\n",
    "    data=agg_results_df,\n",
    "    x=\"rmse_mean\",\n",
    "    y=\"jaccard_mean\",\n",
    "    hue=\"group\",\n",
    "    palette=\"crest\",\n",
    "    s=180,\n",
    "    ax=ax,\n",
    ")\n",
    "for _, row in agg_results_df.iterrows():\n",
    "    ax.text(\n",
    "        row[\"rmse_mean\"],\n",
    "        row[\"jaccard_mean\"] + 0.01,\n",
    "        f\"Team {row['group']}\",\n",
    "        ha=\"center\",\n",
    "    )\n",
    "ax.set_title(\"RMSE vs Jaccard (team means)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.savefig(FIG_DIR / \"rmse_vs_jaccard.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Score distribution per team\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(\n",
    "    data=score_df,\n",
    "    x=\"group\",\n",
    "    y=\"score\",\n",
    "    order=scatter_order,\n",
    "    palette=\"pastel\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Score distribution across tasks\")\n",
    "ax.set_xlabel(\"Team\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "fig.savefig(FIG_DIR / \"score_distribution_by_team.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e0bd7",
   "metadata": {},
   "source": [
    "## Task-wise wins\n",
    "\n",
    "Count how many tasks each team wins (lowest score per task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_winners = score_df.sort_values(\"score\").groupby(\"data_id\").first().reset_index()\n",
    "win_counts = (\n",
    "    task_winners[\"group\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"group\", \"group\": \"wins\"})\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.barplot(data=win_counts, x=\"group\", y=\"wins\", palette=\"coolwarm\", ax=ax)\n",
    "ax.set_title(\"Task wins per team\")\n",
    "ax.set_xlabel(\"Team\")\n",
    "ax.set_ylabel(\"# Wins (lowest score)\")\n",
    "fig.savefig(FIG_DIR / \"task_wins_by_team.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "win_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd18de",
   "metadata": {},
   "source": [
    "## Quick takeaway\n",
    "\n",
    "Highlight the current leader and a headline stat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c015ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leader = agg_results_df.iloc[0]\n",
    "print(\n",
    "    f\"Leader: Team {leader.group} ({leader.members}) — score {leader.score_mean:.4f}, \"\n",
    "    f\"RMSE {leader.rmse_mean:.3f}, Jaccard {leader.jaccard_mean:.3f}.\"\n",
    ")\n",
    "print(f\"RMSE/Jaccard correlation across tasks: {corr_rmse_jaccard:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blanket (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
