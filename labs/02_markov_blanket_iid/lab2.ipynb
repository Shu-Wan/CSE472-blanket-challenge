{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 2: Markov Blanket in the IID Scenario\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading generated datasets with true DAG structure\n",
    "2. Visualizing the true causal graph\n",
    "3. Introduce three feature selection methods and corresponding training receipes:\n",
    "    - Full feature set\n",
    "    - Causal discovery based Markov Blanket search (CD-MB)\n",
    "    - L1 loss\n",
    "    - Oracle Markov Blanket (Oracle-MB)\n",
    "4. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": "%load_ext watermark\n%watermark -a \"Muhammed Hunaid Topiwala\" -v\n\n%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from blanket.feature_selection import (\n",
    "    direct_lingam_selector,\n",
    "    ges_selector,\n",
    "    notears_selector,\n",
    "    pc_selector,\n",
    ")\n",
    "from blanket.models import (\n",
    "    linear_l1_regression,\n",
    "    linear_regression,\n",
    ")\n",
    "\n",
    "from blanket.metrics import adjacency_confusion, jaccard_score, reduction_rate, shd\n",
    "from blanket.plots import plot_adjmat, plot_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Select Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train = load_dataset(path=\"CSE472-blanket-challenge/phase1-dataset\", split='train', name='linear')\n",
    "\n",
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = train[70]\n",
    "\n",
    "X = np.asarray(train_example[\"X\"])\n",
    "y = np.asarray(train_example[\"y\"])\n",
    "adj_mat = np.asarray(train_example[\"adjacency_matrix\"])\n",
    "num_nodes = train_example[\"num_nodes\"]\n",
    "density = train_example[\"density\"]\n",
    "mb = np.asarray(train_example[\"feature_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Visualize True DAG and Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7), constrained_layout=True)\n",
    "\n",
    "plot_graph(adj_mat, figsize=(5, 5), ax=axes[0], title=\"True DAG\")\n",
    "\n",
    "plot_adjmat(\n",
    "    adj_mat,\n",
    "    title=\"Adjacency Matrix\",\n",
    "    figsize=(5, 5),\n",
    "    ax=axes[1],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. PC-based MB Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feature, pc_adjmat = pc_selector(X, y, alpha=0.05, ci_test=\"fisherz\", variant=\"stable\")\n",
    "ges_feature, ges_adjmat = ges_selector(X, y, criterion=\"bic\", method=\"scatter\")\n",
    "direct_lingam_feature, direct_lingam_adjmat = direct_lingam_selector(\n",
    "    X, y, measure=\"pwling\", thresh=0.3\n",
    ")\n",
    "notears_feature, notears_adjmat = notears_selector(X, y, lambda1=0.1, loss_type=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute graph metrics (SHD, precision, recall, F1) and MB metrics for each discovered graph\n",
    "\n",
    "cdmb_results = {\n",
    "    \"PC\": (pc_feature, pc_adjmat),\n",
    "    \"GES\": (ges_feature, ges_adjmat),\n",
    "    \"DirectLiNGAM\": (direct_lingam_feature, direct_lingam_adjmat),\n",
    "    \"NOTEARS\": (notears_feature, notears_adjmat),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, results in cdmb_results.items():\n",
    "    feature = results[0]\n",
    "    adj = results[1]\n",
    "    shd_val = shd(adj_mat, adj)\n",
    "    precision, recall, f1 = adjacency_confusion(adj_mat, adj)\n",
    "\n",
    "    mb_jaccard = jaccard_score(mb, feature)\n",
    "    mb_size = int(np.sum(feature))\n",
    "    mb_reduction = reduction_rate(feature)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Method\": name,\n",
    "            \"SHD\": int(shd_val),\n",
    "            \"Precision\": float(precision),\n",
    "            \"Recall\": float(recall),\n",
    "            \"F1 Score\": float(f1),\n",
    "            \"MB Jaccard\": float(mb_jaccard),\n",
    "            \"MB Size\": mb_size,\n",
    "            \"Reduction Rate\": float(mb_reduction),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create and display comparison table\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "comparison_df.sort_values(by=\"F1 Score\", ascending=False, inplace=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "num_methods = len(cdmb_results)\n",
    "fig, axes = plt.subplots(2, num_methods + 1, figsize=(num_methods * 8, 12), constrained_layout=True)\n",
    "\n",
    "plot_graph(\n",
    "    adj_mat,\n",
    "    figsize=(5, 5),\n",
    "    title=\"True DAG\",\n",
    "    ax=axes[0, 0],\n",
    ")\n",
    "\n",
    "plot_adjmat(\n",
    "    adj_mat,\n",
    "    title=\"True Adjacency Matrix\",\n",
    "    figsize=(5, 5),\n",
    "    ax=axes[1, 0],\n",
    ")\n",
    "\n",
    "for i, (name, results) in enumerate(cdmb_results.items(), 1):\n",
    "    adj = results[1]\n",
    "    plot_graph(\n",
    "        adj,\n",
    "        figsize=(5, 5),\n",
    "        title=f\"{name}\",\n",
    "        ax=axes[0, i],\n",
    "    )\n",
    "\n",
    "    plot_adjmat(\n",
    "        adj,\n",
    "        title=f\"{name}\",\n",
    "        figsize=(5, 5),\n",
    "        ax=axes[1, i],\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Define Training Function and Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "explain three [feature selection methods](https://sebastianraschka.com/faq/docs/feature_sele_categories.html)\n",
    "1. filter: select feature by some metric with a threshold (e.g., $corr(X_i, y) > 0.8$)\n",
    "2. wrapper: Use a wrapper model to perform feature selection. CB-MS\n",
    "3. embedded: feature selection and training are integrated together. L1 regularization\n",
    "\n",
    "We focus on wrapper and embedded methods\n",
    "\n",
    "They have quite different training receipe\n",
    "\n",
    "For Wrapper method\n",
    "1. use the wrapper model to find optimal feature set\n",
    "2. train a model using the feature set\n",
    "\n",
    "For embedded method\n",
    "1. train a model with embedded method\n",
    "2. find optimal feature set\n",
    "3. train the model again with the new feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear models with different feature selections\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n=== Training Linear Models ===\")\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = (X_train, y_train)\n",
    "test_data = (X_test, y_test)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Full features\n",
    "results[\"full\"] = linear_regression(train_data, test_data, feature_mask=None)\n",
    "\n",
    "# 2. CD-MB methods (PC, GES, DirectLiNGAM, NOTEARS)\n",
    "results[\"PC\"] = linear_regression(train_data, test_data, feature_mask=pc_feature)\n",
    "results[\"GES\"] = linear_regression(train_data, test_data, feature_mask=ges_feature)\n",
    "results[\"DirectLiNGAM\"] = linear_regression(train_data, test_data, feature_mask=direct_lingam_feature)\n",
    "results[\"NOTEARS\"] = linear_regression(train_data, test_data, feature_mask=notears_feature)\n",
    "\n",
    "# 3. L1 feature selection\n",
    "l1_result = linear_l1_regression(train_data, test_data, alpha=0.05)\n",
    "results[\"L1\"] = l1_result\n",
    "l1_feature = l1_result[\"feature_mask\"]\n",
    "\n",
    "# 4. Oracle (true Markov Blanket)\n",
    "results[\"Oracle\"] = linear_regression(train_data, test_data, feature_mask=mb)\n",
    "\n",
    "print(f\"Trained {len(results)} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results sorted by RMSE\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'RMSE': [results[k]['rmse'] for k in results.keys()],\n",
    "    'MAE': [results[k]['mae'] for k in results.keys()],\n",
    "    'R2': [results[k]['r2'] for k in results.keys()],\n",
    "    'Features': [results[k]['n_features'] for k in results.keys()]\n",
    "})\n",
    "\n",
    "df_results.sort_values('RMSE', ascending=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. R2 comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_results.plot(x='Model', y='R2', kind='bar', ax=ax1, color='skyblue', legend=False)\n",
    "ax1.set_ylabel('R2 Score', fontweight='bold')\n",
    "ax1.set_title('R2 Score (Higher is Better)', fontweight='bold')\n",
    "ax1.set_xlabel('')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. RMSE comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_results.plot(x='Model', y='RMSE', kind='bar', ax=ax2, color='coral', legend=False)\n",
    "ax2.set_ylabel('RMSE', fontweight='bold')\n",
    "ax2.set_title('Root Mean Squared Error', fontweight='bold')\n",
    "ax2.set_xlabel('')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Predictions vs Ground Truth (Oracle MB)\n",
    "ax3 = axes[1, 0]\n",
    "y_pred_oracle = results['Oracle']['predictions']\n",
    "y_test_oracle = results['Oracle']['truths']\n",
    "ax3.scatter(y_test_oracle, y_pred_oracle, alpha=0.6, s=30)\n",
    "ax3.plot([y_test_oracle.min(), y_test_oracle.max()], [y_test_oracle.min(), y_test_oracle.max()],\n",
    "            'r--', lw=2, label='Perfect prediction')\n",
    "ax3.set_xlabel('True Values', fontweight='bold')\n",
    "ax3.set_ylabel('Predicted Values', fontweight='bold')\n",
    "ax3.set_title('Oracle MB: Predictions vs Ground Truth', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature count vs R2 Performance\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(df_results['Features'], df_results['R2'], s=100, alpha=0.7)\n",
    "for idx, row in df_results.iterrows():\n",
    "    ax4.annotate(row['Model'], (row['Features'], row['R2']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax4.set_xlabel('Number of Features', fontweight='bold')\n",
    "ax4.set_ylabel('R2', fontweight='bold')\n",
    "ax4.set_title('Feature Efficiency', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "From the above, we can see that Oracle MB gives the best prediction result on IID linear setting. However, it may just be a single case.\n",
    "\n",
    "#### 1. Sample Size Analysis\n",
    "**Objective**: Investigate how training data size affects feature selection methods.\n",
    "\n",
    "- Vary the training set sizes (e.g., 50, 100, 500, 800)\n",
    "- Plot RMSE vs. training sample size for each method\n",
    "- Is Orcale MB robust in differernt sizes?\n",
    "- Are CD-MB method robust on varing sizes?\n",
    "\n",
    "#### 2. Nonlinear Relationships (Advanced)\n",
    "**Objective**: Test methods on nonlinear datasets.\n",
    "\n",
    "- Load the `'nonlinear'` subset: `load_dataset(path=\"CSE472-blanket-challenge/phase1-dataset\", split='train', name='nonlinear')`\n",
    "- Evaluate MLP models with feature selection\n",
    "- Compare performance under nonlinear setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Bonus Points [5 points]\n",
    "\n",
    "### Choose one:\n",
    "\n",
    "#### 1. Graph Structure Analysis  \n",
    "**Objective**: Understand how graph properties influence method performance.\n",
    "\n",
    "- Test across graphs with varying:\n",
    "  - **Density**:\n",
    "  - **Markov Blanket size**: MB size w.r.t total feature size\n",
    "  - **Graph size**: number of nodes\n",
    "\n",
    "#### 2. L1 Regularization Tuning\n",
    "**Objective**: Improve L1 feature selection through better hyperparameter selection.\n",
    "\n",
    "- Implement adaptive alpha selection:\n",
    "  - Cross-validation grid search\n",
    "  - Use validation set to choose optimal alpha\n",
    "  - Information criterion (AIC/BIC) based selection\n",
    "- Compare with fixed alpha = 0.05\n",
    "- **Key Question**: Can dynamic alpha selection make L1 competitive with Oracle MB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ega0r9jae5",
   "source": "## 6. Sample Size Analysis\n\nInvestigate how training data size affects feature selection methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0bncsqgup6n",
   "source": "# Sample size analysis\nfrom tqdm.auto import tqdm\n\ntrain_sizes = [50, 100, 500, 800]\nsample_size_results = []\n\n# Use the same example dataset\nprint(\"Running sample size analysis...\")\n\nfor size in tqdm(train_sizes, desc=\"Sample sizes\"):\n    # Subsample training data\n    X_train_sub = X[:size]\n    y_train_sub = y[:size]\n    X_test_sub = X[size:size+200] if size+200 <= len(X) else X[size:]\n    y_test_sub = y[size:size+200] if size+200 <= len(y) else y[size:]\n    \n    train_data_sub = (X_train_sub, y_train_sub)\n    test_data_sub = (X_test_sub, y_test_sub)\n    \n    # Recompute feature selectors on subset\n    pc_feat_sub, _ = pc_selector(X_train_sub, y_train_sub, alpha=0.05, ci_test=\"fisherz\", variant=\"stable\")\n    ges_feat_sub, _ = ges_selector(X_train_sub, y_train_sub, criterion=\"bic\", method=\"scatter\")\n    lingam_feat_sub, _ = direct_lingam_selector(X_train_sub, y_train_sub, measure=\"pwling\", thresh=0.3)\n    notears_feat_sub, _ = notears_selector(X_train_sub, y_train_sub, lambda1=0.1, loss_type=\"l2\")\n    \n    # Test different methods\n    methods = {\n        'Full': None,\n        'Oracle MB': mb,\n        'PC': pc_feat_sub,\n        'GES': ges_feat_sub,\n        'DirectLiNGAM': lingam_feat_sub,\n        'NOTEARS': notears_feat_sub,\n    }\n    \n    # L1 method\n    l1_res = linear_l1_regression(train_data_sub, test_data_sub, alpha=0.05)\n    methods['L1'] = l1_res['feature_mask']\n    \n    for method_name, feature_mask in methods.items():\n        result = linear_regression(train_data_sub, test_data_sub, feature_mask=feature_mask)\n        sample_size_results.append({\n            'Train Size': size,\n            'Method': method_name,\n            'RMSE': result['rmse'],\n            'MAE': result['mae'],\n            'R2': result['r2'],\n            'N Features': result['n_features']\n        })\n\nsample_size_df = pd.DataFrame(sample_size_results)\nprint(\"Sample size analysis complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yts1ixo9p1p",
   "source": "# Visualize sample size analysis\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\n# Plot RMSE vs sample size\nfor method in sample_size_df['Method'].unique():\n    method_data = sample_size_df[sample_size_df['Method'] == method]\n    axes[0].plot(method_data['Train Size'], method_data['RMSE'], marker='o', label=method, linewidth=2)\n\naxes[0].set_xlabel('Training Sample Size', fontweight='bold')\naxes[0].set_ylabel('RMSE', fontweight='bold')\naxes[0].set_title('RMSE vs Training Sample Size', fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot R2 vs sample size\nfor method in sample_size_df['Method'].unique():\n    method_data = sample_size_df[sample_size_df['Method'] == method]\n    axes[1].plot(method_data['Train Size'], method_data['R2'], marker='o', label=method, linewidth=2)\n\naxes[1].set_xlabel('Training Sample Size', fontweight='bold')\naxes[1].set_ylabel('R² Score', fontweight='bold')\naxes[1].set_title('R² vs Training Sample Size', fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Display summary table\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAMPLE SIZE ANALYSIS SUMMARY\")\nprint(\"=\"*80)\nsummary = sample_size_df.groupby(['Method', 'Train Size'])['RMSE'].mean().unstack()\nprint(summary)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "oe3rr6rj6n",
   "source": "# Load nonlinear dataset\nfrom blanket.models import mlp_regression\n\ntrain_nonlinear = load_dataset(path=\"CSE472-blanket-challenge/phase1-dataset\", split='train', name='nonlinear')\n\n# Select same index for fair comparison\nnonlinear_example = train_nonlinear[70]\n\nX_nl = np.asarray(nonlinear_example[\"X\"])\ny_nl = np.asarray(nonlinear_example[\"y\"])\nadj_mat_nl = np.asarray(nonlinear_example[\"adjacency_matrix\"])\nmb_nl = np.asarray(nonlinear_example[\"feature_mask\"])\n\nprint(f\"Nonlinear dataset loaded: {X_nl.shape}\")\n\n# Create train/test split\nX_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(X_nl, y_nl, test_size=0.2, random_state=42)\ntrain_data_nl = (X_train_nl, y_train_nl)\ntest_data_nl = (X_test_nl, y_test_nl)\n\n# Compute feature selectors on nonlinear data\nprint(\"Computing feature selectors...\")\npc_feat_nl, _ = pc_selector(X_train_nl, y_train_nl, alpha=0.05, ci_test=\"fisherz\", variant=\"stable\")\nges_feat_nl, _ = ges_selector(X_train_nl, y_train_nl, criterion=\"bic\", method=\"scatter\")\nlingam_feat_nl, _ = direct_lingam_selector(X_train_nl, y_train_nl, measure=\"pwling\", thresh=0.3)\nnotears_feat_nl, _ = notears_selector(X_train_nl, y_train_nl, lambda1=0.1, loss_type=\"l2\")\nl1_res_nl = linear_l1_regression(train_data_nl, test_data_nl, alpha=0.05)\n\nprint(\"Feature selection complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p3vpw7s41k",
   "source": "# Compare Linear vs MLP performance on nonlinear data\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Comparison: Linear (from earlier) vs MLP (nonlinear data)\ncomparison = pd.DataFrame({\n    'Method': df_results_nl['Model'],\n    'MLP (Nonlinear)': df_results_nl['RMSE'],\n    'Linear (From Section 5)': df_results['RMSE']  # From linear results earlier\n})\n\ncomparison.plot(x='Method', kind='bar', ax=axes[0], rot=45)\naxes[0].set_ylabel('RMSE', fontweight='bold')\naxes[0].set_title('RMSE: MLP vs Linear Models', fontweight='bold')\naxes[0].set_xlabel('')\naxes[0].legend(['MLP (Nonlinear Data)', 'Linear (Linear Data)'])\naxes[0].grid(True, alpha=0.3)\n\n# R2 comparison\ncomparison_r2 = pd.DataFrame({\n    'Method': df_results_nl['Model'],\n    'MLP (Nonlinear)': df_results_nl['R2'],\n    'Linear (From Section 5)': df_results['R2']\n})\n\ncomparison_r2.plot(x='Method', kind='bar', ax=axes[1], rot=45)\naxes[1].set_ylabel('R² Score', fontweight='bold')\naxes[1].set_title('R²: MLP vs Linear Models', fontweight='bold')\naxes[1].set_xlabel('')\naxes[1].legend(['MLP (Nonlinear Data)', 'Linear (Linear Data)'])\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uz7l2p4ah3",
   "source": "# Train MLP models with different feature selections\nresults_nl = {}\n\nprint(\"\\n=== Training MLP Models on Nonlinear Data ===\")\n\n# 1. Full features\nresults_nl[\"Full\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=None)\n\n# 2. CD-MB methods\nresults_nl[\"PC\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=pc_feat_nl)\nresults_nl[\"GES\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=ges_feat_nl)\nresults_nl[\"DirectLiNGAM\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=lingam_feat_nl)\nresults_nl[\"NOTEARS\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=notears_feat_nl)\n\n# 3. L1\nresults_nl[\"L1\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=l1_res_nl['feature_mask'])\n\n# 4. Oracle MB\nresults_nl[\"Oracle MB\"] = mlp_regression(train_data_nl, test_data_nl, feature_mask=mb_nl)\n\n# Create results dataframe\ndf_results_nl = pd.DataFrame({\n    'Model': list(results_nl.keys()),\n    'RMSE': [results_nl[k]['rmse'] for k in results_nl.keys()],\n    'MAE': [results_nl[k]['mae'] for k in results_nl.keys()],\n    'R2': [results_nl[k]['r2'] for k in results_nl.keys()],\n    'Features': [results_nl[k]['n_features'] for k in results_nl.keys()]\n})\n\nprint(f\"\\nTrained {len(results_nl)} MLP models\")\ndf_results_nl.sort_values('RMSE', ascending=True).reset_index(drop=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z72rztmkhr",
   "source": "## 7. Nonlinear Relationships\n\nTest feature selection methods on nonlinear datasets using MLP models.",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}