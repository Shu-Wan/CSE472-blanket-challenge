{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 2: Markov Blanket in the IID Scenario\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading generated datasets with true DAG structure\n",
    "2. Visualizing the true causal graph\n",
    "3. Introduce three feature selection methods and corresponding training receipes:\n",
    "    - Full feature set\n",
    "    - Causal discovery based Markov Blanket search (CD-MB)\n",
    "    - L1 loss\n",
    "    - Oracle Markov Blanket (Oracle-MB)\n",
    "4. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Shu Wan\" -v\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from blanket.feature_selection import (\n",
    "    direct_lingam_selector,\n",
    "    ges_selector,\n",
    "    notears_selector,\n",
    "    pc_selector,\n",
    ")\n",
    "from blanket.models import (\n",
    "    linear_l1_regression,\n",
    "    linear_regression,\n",
    ")\n",
    "\n",
    "from blanket.metrics import adjacency_confusion, jaccard_score, reduction_rate, shd\n",
    "from blanket.plots import plot_adjmat, plot_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Select Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train = load_dataset(path=\"CSE472-blanket-challenge/phase1-dataset\", split='train', name='linear')\n",
    "\n",
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = train[70]\n",
    "\n",
    "X = np.asarray(train_example[\"X\"])\n",
    "y = np.asarray(train_example[\"y\"])\n",
    "adj_mat = np.asarray(train_example[\"adjacency_matrix\"])\n",
    "num_nodes = train_example[\"num_nodes\"]\n",
    "density = train_example[\"density\"]\n",
    "mb = np.asarray(train_example[\"feature_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Visualize True DAG and Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7), constrained_layout=True)\n",
    "\n",
    "plot_graph(adj_mat, figsize=(5, 5), ax=axes[0], title=\"True DAG\")\n",
    "\n",
    "plot_adjmat(\n",
    "    adj_mat,\n",
    "    title=\"Adjacency Matrix\",\n",
    "    figsize=(5, 5),\n",
    "    ax=axes[1],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. PC-based MB Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feature, pc_adjmat = pc_selector(X, y, alpha=0.05, ci_test=\"fisherz\", variant=\"stable\")\n",
    "ges_feature, ges_adjmat = ges_selector(X, y, criterion=\"bic\", method=\"scatter\")\n",
    "direct_lingam_feature, direct_lingam_adjmat = direct_lingam_selector(\n",
    "    X, y, measure=\"pwling\", thresh=0.3\n",
    ")\n",
    "notears_feature, notears_adjmat = notears_selector(X, y, lambda1=0.1, loss_type=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute graph metrics (SHD, precision, recall, F1) and MB metrics for each discovered graph\n",
    "\n",
    "cdmb_results = {\n",
    "    \"PC\": (pc_feature, pc_adjmat),\n",
    "    \"GES\": (ges_feature, ges_adjmat),\n",
    "    \"DirectLiNGAM\": (direct_lingam_feature, direct_lingam_adjmat),\n",
    "    \"NOTEARS\": (notears_feature, notears_adjmat),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, results in cdmb_results.items():\n",
    "    feature = results[0]\n",
    "    adj = results[1]\n",
    "    shd_val = shd(adj_mat, adj)\n",
    "    precision, recall, f1 = adjacency_confusion(adj_mat, adj)\n",
    "\n",
    "    mb_jaccard = jaccard_score(mb, feature)\n",
    "    mb_size = int(np.sum(feature))\n",
    "    mb_reduction = reduction_rate(feature)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Method\": name,\n",
    "            \"SHD\": int(shd_val),\n",
    "            \"Precision\": float(precision),\n",
    "            \"Recall\": float(recall),\n",
    "            \"F1 Score\": float(f1),\n",
    "            \"MB Jaccard\": float(mb_jaccard),\n",
    "            \"MB Size\": mb_size,\n",
    "            \"Reduction Rate\": float(mb_reduction),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create and display comparison table\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "comparison_df.sort_values(by=\"F1 Score\", ascending=False, inplace=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "num_methods = len(cdmb_results)\n",
    "fig, axes = plt.subplots(2, num_methods + 1, figsize=(num_methods * 8, 12), constrained_layout=True)\n",
    "\n",
    "plot_graph(\n",
    "    adj_mat,\n",
    "    figsize=(5, 5),\n",
    "    title=\"True DAG\",\n",
    "    ax=axes[0, 0],\n",
    ")\n",
    "\n",
    "plot_adjmat(\n",
    "    adj_mat,\n",
    "    title=\"True Adjacency Matrix\",\n",
    "    figsize=(5, 5),\n",
    "    ax=axes[1, 0],\n",
    ")\n",
    "\n",
    "for i, (name, results) in enumerate(cdmb_results.items(), 1):\n",
    "    adj = results[1]\n",
    "    plot_graph(\n",
    "        adj,\n",
    "        figsize=(5, 5),\n",
    "        title=f\"{name}\",\n",
    "        ax=axes[0, i],\n",
    "    )\n",
    "\n",
    "    plot_adjmat(\n",
    "        adj,\n",
    "        title=f\"{name}\",\n",
    "        figsize=(5, 5),\n",
    "        ax=axes[1, i],\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Define Training Function and Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "explain three [feature selection methods](https://sebastianraschka.com/faq/docs/feature_sele_categories.html)\n",
    "1. filter: select feature by some metric with a threshold (e.g., $corr(X_i, y) > 0.8$)\n",
    "2. wrapper: Use a wrapper model to perform feature selection. CB-MS\n",
    "3. embedded: feature selection and training are integrated together. L1 regularization\n",
    "\n",
    "We focus on wrapper and embedded methods\n",
    "\n",
    "They have quite different training receipe\n",
    "\n",
    "For Wrapper method\n",
    "1. use the wrapper model to find optimal feature set\n",
    "2. train a model using the feature set\n",
    "\n",
    "For embedded method\n",
    "1. train a model with embedded method\n",
    "2. find optimal feature set\n",
    "3. train the model again with the new feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear models with different feature selections\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n=== Training Linear Models ===\")\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = (X_train, y_train)\n",
    "test_data = (X_test, y_test)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Full features\n",
    "results[\"full\"] = linear_regression(train_data, test_data, feature_mask=None)\n",
    "\n",
    "# 2. CD-MB methods (PC, GES, DirectLiNGAM, NOTEARS)\n",
    "results[\"PC\"] = linear_regression(train_data, test_data, feature_mask=pc_feature)\n",
    "results[\"GES\"] = linear_regression(train_data, test_data, feature_mask=ges_feature)\n",
    "results[\"DirectLiNGAM\"] = linear_regression(train_data, test_data, feature_mask=direct_lingam_feature)\n",
    "results[\"NOTEARS\"] = linear_regression(train_data, test_data, feature_mask=notears_feature)\n",
    "\n",
    "# 3. L1 feature selection\n",
    "l1_result = linear_l1_regression(train_data, test_data, alpha=0.05)\n",
    "results[\"L1\"] = l1_result\n",
    "l1_feature = l1_result[\"feature_mask\"]\n",
    "\n",
    "# 4. Oracle (true Markov Blanket)\n",
    "results[\"Oracle\"] = linear_regression(train_data, test_data, feature_mask=mb)\n",
    "\n",
    "print(f\"Trained {len(results)} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results sorted by RMSE\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'RMSE': [results[k]['rmse'] for k in results.keys()],\n",
    "    'MAE': [results[k]['mae'] for k in results.keys()],\n",
    "    'R2': [results[k]['r2'] for k in results.keys()],\n",
    "    'Features': [results[k]['n_features'] for k in results.keys()]\n",
    "})\n",
    "\n",
    "df_results.sort_values('RMSE', ascending=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. R2 comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_results.plot(x='Model', y='R2', kind='bar', ax=ax1, color='skyblue', legend=False)\n",
    "ax1.set_ylabel('R2 Score', fontweight='bold')\n",
    "ax1.set_title('R2 Score (Higher is Better)', fontweight='bold')\n",
    "ax1.set_xlabel('')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. RMSE comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_results.plot(x='Model', y='RMSE', kind='bar', ax=ax2, color='coral', legend=False)\n",
    "ax2.set_ylabel('RMSE', fontweight='bold')\n",
    "ax2.set_title('Root Mean Squared Error', fontweight='bold')\n",
    "ax2.set_xlabel('')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Predictions vs Ground Truth (Oracle MB)\n",
    "ax3 = axes[1, 0]\n",
    "y_pred_oracle = results['Oracle']['predictions']\n",
    "y_test_oracle = results['Oracle']['truths']\n",
    "ax3.scatter(y_test_oracle, y_pred_oracle, alpha=0.6, s=30)\n",
    "ax3.plot([y_test_oracle.min(), y_test_oracle.max()], [y_test_oracle.min(), y_test_oracle.max()],\n",
    "            'r--', lw=2, label='Perfect prediction')\n",
    "ax3.set_xlabel('True Values', fontweight='bold')\n",
    "ax3.set_ylabel('Predicted Values', fontweight='bold')\n",
    "ax3.set_title('Oracle MB: Predictions vs Ground Truth', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature count vs R2 Performance\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(df_results['Features'], df_results['R2'], s=100, alpha=0.7)\n",
    "for idx, row in df_results.iterrows():\n",
    "    ax4.annotate(row['Model'], (row['Features'], row['R2']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax4.set_xlabel('Number of Features', fontweight='bold')\n",
    "ax4.set_ylabel('R2', fontweight='bold')\n",
    "ax4.set_title('Feature Efficiency', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "From the above, we can see that Oracle MB gives the best prediction result on IID linear setting. However, it may just be a single case.\n",
    "\n",
    "#### 1. Sample Size Analysis\n",
    "**Objective**: Investigate how training data size affects feature selection methods.\n",
    "\n",
    "- Vary the training set sizes (e.g., 50, 100, 500, 800)\n",
    "- Plot RMSE vs. training sample size for each method\n",
    "- Is Orcale MB robust in differernt sizes?\n",
    "- Are CD-MB method robust on varing sizes?\n",
    "\n",
    "#### 2. Nonlinear Relationships (Advanced)\n",
    "**Objective**: Test methods on nonlinear datasets.\n",
    "\n",
    "- Load the `'nonlinear'` subset: `load_dataset(path=\"CSE472-blanket-challenge/phase1-dataset\", split='train', name='nonlinear')`\n",
    "- Evaluate MLP models with feature selection\n",
    "- Compare performance under nonlinear setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Bonus Points [5 points]\n",
    "\n",
    "### Choose one:\n",
    "\n",
    "#### 1. Graph Structure Analysis  \n",
    "**Objective**: Understand how graph properties influence method performance.\n",
    "\n",
    "- Test across graphs with varying:\n",
    "  - **Density**:\n",
    "  - **Markov Blanket size**: MB size w.r.t total feature size\n",
    "  - **Graph size**: number of nodes\n",
    "\n",
    "#### 2. L1 Regularization Tuning\n",
    "**Objective**: Improve L1 feature selection through better hyperparameter selection.\n",
    "\n",
    "- Implement adaptive alpha selection:\n",
    "  - Cross-validation grid search\n",
    "  - Use validation set to choose optimal alpha\n",
    "  - Information criterion (AIC/BIC) based selection\n",
    "- Compare with fixed alpha = 0.05\n",
    "- **Key Question**: Can dynamic alpha selection make L1 competitive with Oracle MB?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
