{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 3: MB in OOD settings\n",
    "\n",
    "[Nastl et al.](https://proceedings.neurips.cc/paper_files/paper/2024/file/3792ddbf94b68ff4369f510f7a3e1777-Paper-Conference.pdf) shows \n",
    "that models using all features consistently outperform causal features. \n",
    "\n",
    "These empirical results challenge the practical value of causal features.\n",
    "\n",
    "However, from our previous labs (Lab 1 and 2), we have shown that Markov Blanket is the **optimal** feature set under IID conditions.\n",
    "\n",
    "This apparent contradiction leads to the central question for Lab 3:\n",
    "\n",
    "<center>Is the MB still the optimal feature set in OOD setting?</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Shu Wan\" -v\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from blanket.datasets import load_data\n",
    "from blanket.graph import get_markov_blanket, get_parents, get_children, get_spouses\n",
    "from blanket.plots import plot_graph, plot_distributions\n",
    "from blanket.models import linear_regression\n",
    "\n",
    "load_dotenv()\n",
    "disable_progress_bars()\n",
    "plt.rcParams['figure.dpi'] = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Dataset from HuggingFace\n",
    "\n",
    "In this section, we load the phase3 dataset from HuggingFace, which contains multiple datasets with different environment configurations (IID, covariate shift, and label shift).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from HuggingFace\n",
    "path_to_data = snapshot_download(\n",
    "    repo_id=\"CSE472-blanket-challenge/phase3-dataset\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "# path_to_data = LOCAL_PATH # Uncomment this line to use local data\n",
    "print(f\"Dataset downloaded to: {path_to_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_ds = load_dataset(path_to_data, split='train')\n",
    "print(f\"Total datasets available: {len(metadata_ds)}\")\n",
    "print(f\"\\nMetadata fields: {metadata_ds.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution of environment types\n",
    "env_methods = [entry['environment']['method'] for entry in metadata_ds]\n",
    "env_counts = pd.Series(env_methods).value_counts()\n",
    "print(\"\\nEnvironment distribution:\")\n",
    "env_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific dataset to verify I/O\n",
    "example_data_id = metadata_ds[0]['data_id']\n",
    "print(f\"Loading dataset: {example_data_id}\")\n",
    "\n",
    "X_train, y_train, X_test, y_test, metadata = load_data(\n",
    "    path_to_data,\n",
    "    data_id=example_data_id,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\nMetadata keys: {list(metadata.keys())}\")\n",
    "print(f\"SCM: {metadata['scm']}\")\n",
    "print(f\"Environment: {metadata['environment']}\")\n",
    "print(f\"Graph Metadata: {metadata['graph_meta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets\n",
    "# criteria: has parents, children, not all features are in mb, linear functional\n",
    "datasets = {}\n",
    "example_meta = metadata_ds.filter(lambda x: sum(x['parents']) > 0 and sum(x['children']) > 0 and x['scm']['functional'] == 'linear' and sum(x['feature_mask']) < x['n_features'])\n",
    "\n",
    "for idx, data_id in enumerate(example_meta['data_id']):\n",
    "    X_tr, y_tr, X_te, y_te, meta = load_data(path_to_data, data_id=data_id, include_metadata=True)\n",
    "    shift = meta['environment']['method']\n",
    "    key = f\"{shift}-{idx}\"\n",
    "    datasets[key] = {\n",
    "        'X_train': X_tr, 'y_train': y_tr,\n",
    "        'X_test': X_te, 'y_test': y_te,\n",
    "        'metadata': meta\n",
    "    }\n",
    "\n",
    "print(\"Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. IID vs OOD: Understanding the Difference\n",
    "\n",
    "### IID (Independent and Identically Distributed):\n",
    "- Training and test data are drawn from the same distribution\n",
    "- $P_{\\text{train}}(X, Y) = P_{\\text{test}}(X, Y)$\n",
    "- Standard assumption in machine learning\n",
    "\n",
    "### Covariate Shift (OOD)\n",
    "- The distribution of input features changes: $P_{\\text{train}}(X) \\neq P_{\\text{test}}(X)$\n",
    "- The conditional distribution remains the same: $P_{\\text{train}}(Y|X) = P_{\\text{test}}(Y|X)$\n",
    "\n",
    "### Label Shift (OOD)\n",
    "- The distribution of the target changes: $P_{\\text{train}}(Y) \\neq P_{\\text{test}}(Y)$\n",
    "- The conditional distribution $P(X|Y)$ remains the same: $P_{\\text{train}}(X|Y) = P_{\\text{test}}(X|Y)$\n",
    "\n",
    "Let's visualize the difference by comparing datasets with different environment types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_index = [i for i, key in enumerate(datasets.keys()) if key.split('-')[0] == 'iid']\n",
    "covariate_index = [i for i, key in enumerate(datasets.keys()) if key.split('-')[0] == 'covariate']\n",
    "label_index = [i for i, key in enumerate(datasets.keys()) if key.split('-')[0] == 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_datasets = {\n",
    "    'iid': datasets[f'iid-{iid_index[0]}'],\n",
    "    'covariate': datasets[f'covariate-{covariate_index[0]}'],\n",
    "    'label': datasets[f'label-{label_index[0]}']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution differences\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "\n",
    "for i, (name, data) in enumerate(example_datasets.items()):\n",
    "    # Plot feature distribution\n",
    "    plot_distributions(\n",
    "        data['X_train'],\n",
    "        data['X_test'],\n",
    "        labels=('Train', 'Test'),\n",
    "        ax=axes[i, 0],\n",
    "        title=f\"{name}: P(X)\"\n",
    "    )\n",
    "\n",
    "    # Plot target distribution\n",
    "    plot_distributions(\n",
    "        data['y_train'].reshape(-1, 1),\n",
    "        data['y_test'].reshape(-1, 1),\n",
    "        labels=('Train', 'Test'),\n",
    "        ax=axes[i, 1],\n",
    "        title=f\"{name}: P(y)\"\n",
    "    )\n",
    "\n",
    "    # Plot joint distribution\n",
    "    plot_distributions(\n",
    "        np.hstack([data['X_train'], data['y_train'].reshape(-1, 1)]),\n",
    "        np.hstack([data['X_test'], data['y_test'].reshape(-1, 1)]),\n",
    "        labels=('Train', 'Test'),\n",
    "        ax=axes[i, 2],\n",
    "        title=f\"{name}: P(X, y)\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify distribution differences using KS statistic\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for name, data in example_datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # KS test for features (average over all features)\n",
    "    feature_ks = []\n",
    "    for j in range(data['X_train'].shape[1]):\n",
    "        ks_stat, _ = ks_2samp(data['X_train'][:, j], data['X_test'][:, j])\n",
    "        feature_ks.append(ks_stat)\n",
    "\n",
    "    # KS test for target\n",
    "    target_ks, _ = ks_2samp(data['y_train'], data['y_test'])\n",
    "\n",
    "    print(f\"  Average Feature KS statistic: {np.mean(feature_ks):.4f}\")\n",
    "    print(f\"  Target KS statistic: {target_ks:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a DAG showing different feature types\n",
    "# Using one of our datasets\n",
    "example_name = 'covariate'\n",
    "example_data = example_datasets[example_name]\n",
    "adj_matrix = np.asarray(example_data['metadata']['adjacency_matrix'])\n",
    "\n",
    "print(f\"Number of nodes: {adj_matrix.shape[0]}\")\n",
    "\n",
    "parents = get_parents(adj_matrix)\n",
    "children = get_children(adj_matrix)\n",
    "spouses = get_spouses(adj_matrix)\n",
    "mb_mask = example_data['metadata']['feature_mask']\n",
    "\n",
    "print(f\"\\nParents: {np.where(parents)[0].tolist()}\")\n",
    "print(f\"Children: {np.where(children)[0].tolist()}\")\n",
    "print(f\"Spouses: {np.where(spouses)[0].tolist()}\")\n",
    "print(f\"Markov Blanket: {np.where(mb_mask)[0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the DAG\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_graph(adj_matrix, ax=ax)\n",
    "ax.set_title(f'Example DAG')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 3. Optimality of Markov Blanket in OOD setting\n",
    "\n",
    "\n",
    "We consider the following feature sets:\n",
    "\n",
    "1. All Features\n",
    "2. Markov Blanket\n",
    "3. Parents (causal)\n",
    "4. Children (anticausal)\n",
    "5. P + C\n",
    "6. Children + Spouses\n",
    "\n",
    "These feature sets are tested across:\n",
    "- Three environment types: IID, Covariate Shift, Label Shift\n",
    "- Varying training sizes: 50 to 800, every 50 as a step\n",
    "- Evaluation metric: Test MAE and STD (bias and variance)\n",
    "\n",
    "Expected Outcome\n",
    "\n",
    "- Nastl et al. show that all features outperforms parents and children consistently and by a large margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_sets(adj_matrix):\n",
    "    \"\"\"Get different feature selection masks.\"\"\"\n",
    "    n_features = adj_matrix.shape[0]\n",
    "    feature_sets = {}\n",
    "\n",
    "    feature_sets['All'] = np.ones(n_features, dtype=int)\n",
    "    feature_sets['MB'] = get_markov_blanket(adj_matrix, -1)\n",
    "    feature_sets['Parents'] = get_parents(adj_matrix, -1)\n",
    "    feature_sets['Children'] = get_children(adj_matrix, -1)\n",
    "    feature_sets['Parents+Children'] = feature_sets['Parents'] | feature_sets['Children']\n",
    "    feature_sets['Children+Spouses'] = get_children(adj_matrix, -1) | get_spouses(adj_matrix, -1)\n",
    "\n",
    "    # drop last element (target)\n",
    "    for key in feature_sets:\n",
    "        feature_sets[key] = feature_sets[key][:-1]\n",
    "    return feature_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_set(X_train, y_train, X_test, y_test, feature_mask):\n",
    "    \"\"\"Train and evaluate a model with selected features.\"\"\"\n",
    "    # Check if any features are selected\n",
    "    assert np.any(feature_mask)\n",
    "\n",
    "    # Train model\n",
    "    results = linear_regression((X_train, y_train), (X_test, y_test), feature_mask)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "train_sizes = list(range(50, 801, 50))\n",
    "results = []\n",
    "\n",
    "for env_name, data in datasets.items():\n",
    "\n",
    "    adj_matrix = np.asarray(data['metadata']['adjacency_matrix'])\n",
    "\n",
    "    X_train_full = data['X_train']\n",
    "    y_train_full = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "    for train_size in train_sizes:\n",
    "        # Subsample training data\n",
    "        X_train = X_train_full[:train_size]\n",
    "        y_train = y_train_full[:train_size]\n",
    "\n",
    "        # Get feature sets\n",
    "        feature_sets = get_feature_sets(adj_matrix)\n",
    "\n",
    "        # Evaluate each feature set\n",
    "        for fs_name, fs_mask in feature_sets.items():\n",
    "            metrics = evaluate_feature_set(X_train, y_train, X_test, y_test, fs_mask)\n",
    "\n",
    "            results.append({\n",
    "                'id': env_name,\n",
    "                'Environment': env_name.split('-')[0],\n",
    "                'Train Size': train_size,\n",
    "                'Feature Set': fs_name,\n",
    "                'RMSE': metrics['rmse'],\n",
    "                'MAE': metrics['mae'],\n",
    "                'STD': metrics['std'],\n",
    "                'N Features': int(np.sum(fs_mask))\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nEvaluation complete!\")\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = results_df.groupby(['Environment', 'Feature Set', 'Train Size']).agg({\n",
    "    'RMSE': 'mean',\n",
    "    'MAE': 'mean',\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for i, env_name in enumerate(['iid', 'covariate', 'label']):\n",
    "    env_results = summary_df[summary_df['Environment'] == env_name]\n",
    "\n",
    "    # Plot RMSE\n",
    "    for fs_name in ['All', 'MB', 'Parents', 'Children', 'Parents+Children', 'Children+Spouses']:\n",
    "        fs_results = env_results[env_results['Feature Set'] == fs_name]\n",
    "        axes[0, i].plot(fs_results['Train Size'], fs_results['RMSE'],\n",
    "                       marker='o', label=fs_name, linewidth=2)\n",
    "\n",
    "    axes[0, i].set_xlabel('Training Size')\n",
    "    axes[0, i].set_ylabel('Test RMSE')\n",
    "    axes[0, i].set_title(f'{env_name}: Test RMSE')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot MAE\n",
    "    for fs_name in ['All', 'MB', 'Parents', 'Children', 'Parents+Children', 'Children+Spouses']:\n",
    "        fs_results = env_results[env_results['Feature Set'] == fs_name]\n",
    "        axes[1, i].plot(fs_results['Train Size'], fs_results['MAE'],\n",
    "                       marker='o', label=fs_name, linewidth=2)\n",
    "\n",
    "    axes[1, i].set_xlabel('Training Size')\n",
    "    axes[1, i].set_ylabel('Test MAE')\n",
    "    axes[1, i].set_title(f'{env_name}: Test MAE')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 4. Key findings\n",
    "\n",
    "1. MB is still the optimal feature set in OOD settings (at least covariate shift + label shift)\n",
    "2. Parents and Children alone perform significantly worse than other feature sets. This is aligned with results in Nastl et. al\n",
    "3. MB has significant lead when training size (<100) is limited, the gap diminishes as training size grows.\n",
    "4. Due to the small graph size (node < 50) and relative large density, MB is often close to all features, making the difference less evident.\n",
    "\n",
    "### References:\n",
    "\n",
    "Nastl, Vivian, and Moritz Hardt. \"Do causal predictors generalize better to new domains?.\" Advances in Neural Information Processing Systems 37 (2024): 31202-31315."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Bonus Point (5 Points)\n",
    "\n",
    "1. Is MB still optimal for nonlinear functionals? \n",
    "\n",
    "```python\n",
    "example_meta = metadata_ds.filter(lambda x: sum(x['parents']) > 0 and sum(x['children']) > 0 and x['scm']['functional'] == 'nonlinear' and sum(x['feature_mask']) < x['n_features'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE472-blanket-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
