{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Final Project - example solution\n",
    "\n",
    "This notebook demonstrates the integration of TabPFN for two tasks:\n",
    "1.  **Regression**: Predicting the target variable `y`.\n",
    "2.  **Markov Blanket (MB) Discovery**: Identifying the optimal feature set using TabPFN embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from blanket.plots import plot_graph\n",
    "from blanket.metrics import rmse, jaccard_score\n",
    "\n",
    "from tabpfn import TabPFNRegressor\n",
    "from tabpfn_extensions.embedding import TabPFNEmbedding\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "develop = load_dataset(\"CSE472-blanket-challenge/final-dataset\", 'develop', split='train')\n",
    "submit = load_dataset(\"CSE472-blanket-challenge/final-dataset\", 'submit', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(develop), len(submit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "[\ud83e\udd17 Dataset](CSE472-blanket-challenge/final-dataset)\n",
    "\n",
    "Develop: 182 datasets\n",
    "\n",
    "- X_train, y_train, X_test, y_test, metadata\n",
    "\n",
    "Subumit: 46 datasets\n",
    "\n",
    "- X_train, y_train, X_test\n",
    "\n",
    "Develop and Submit use the same script for data generation.\n",
    "\n",
    "For generation details, refer to <https://huggingface.co/datasets/CSE472-blanket-challenge/final-dataset>\n",
    "\n",
    "You task:\n",
    "\n",
    "1. Train a model using `develop` to predict `y` and `markov_blanket`\n",
    "2. Test your model on `submit`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Test Case\n",
    "example_data = develop[89]\n",
    "X_train = np.asarray(example_data['X_train'])\n",
    "y_train = np.asarray(example_data['y_train'])\n",
    "X_test = np.asarray(example_data['X_test'])\n",
    "y_test = np.asarray(example_data['y_test'])\n",
    "\n",
    "print(f\"Example data id: {example_data['data_id']}\")\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")\n",
    "print(f\"Env: {example_data['environment']}\")\n",
    "print(f\"SCM: {example_data['scm']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Causal Graph\n",
    "plot_graph(example_data['adjacency_matrix'], title=f\"Causal Graph: {example_data['graph_id']}\", figsize=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## TabPFN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.model_loading import ModelSource\n",
    "regressor_models = ModelSource.get_regressor_v2_5()\n",
    "\n",
    "print(\"Available TabPFN Regressor Models:\\n\", )\n",
    "for model_name in regressor_models.filenames:\n",
    "    print(f\"  {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "TabPFN load `tabpfn-v2.5-regressor-v2.5_default.ckpt` by default\n",
    "\n",
    "the `real` variant are fine-tuned on real world dataset\n",
    "\n",
    "For details, see TabPFN's [technical report](https://storage.googleapis.com/prior-labs-tabpfn-public/reports/TabPFN_2_5_tech_report.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "TABPFN_MODEL_CACHE_DIR = Path(os.getenv(\"TABPFN_MODEL_CACHE_DIR\", None))\n",
    "\n",
    "model_path = hf_hub_download(repo_id=regressor_models.repo_id, filename=\"tabpfn-v2.5-regressor-v2.5_real.ckpt\", local_dir=TABPFN_MODEL_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### TabPFN toy example\n",
    "mode 1: in context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabpfn is a subclass of sklearn estimators, so it has the same API\n",
    "# toy example\n",
    "regressor_config = {\n",
    "        \"ignore_pretraining_limits\": True,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"n_estimators\": 24,\n",
    "        \"random_state\":42,\n",
    "        \"inference_precision\": \"auto\"\n",
    "    }\n",
    "\n",
    "regressor = TabPFNRegressor(model_path = model_path, **regressor_config)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "preds = regressor.predict(X_test)\n",
    "\n",
    "print(\"RMSE:\", rmse(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "mode 2: fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.utils import meta_dataset_collator\n",
    "from tabpfn.finetune_utils import clone_model_for_evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "regressor_config = {\n",
    "        \"ignore_pretraining_limits\": True,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"n_estimators\": 24,\n",
    "        \"random_state\":42,\n",
    "        \"inference_precision\": \"auto\"\n",
    "    }\n",
    "\n",
    "# --- Setup model ---\n",
    "regressor = TabPFNRegressor(\n",
    "    **regressor_config,\n",
    "    fit_mode=\"batched\", differentiable_input=False\n",
    ")\n",
    "# initialize model weights\n",
    "regressor._initialize_model_variables()\n",
    "\n",
    "optimizer = Adam(regressor.model_.parameters(), lr=1.5e-6)\n",
    "\n",
    "# --- Dataloader setup ---\n",
    "datasets = regressor.get_preprocessed_datasets(X_train, y_train, train_test_split, 10000)\n",
    "loader = DataLoader(datasets, batch_size=1, collate_fn=meta_dataset_collator)\n",
    "\n",
    "# --- Fine-tuning loop ---\n",
    "for epoch in tqdm(range(10), desc=\"Fine-tuning Epochs\", leave=False):\n",
    "    for data_batch in tqdm(loader, desc=f\"Epoch {epoch}\"):\n",
    "        optimizer.zero_grad()\n",
    "        (X_tr, X_te, y_tr, y_te, cat_ixs, confs, raw_space, znorm_space, _, _) = data_batch\n",
    "        regressor.raw_space_bardist_ = raw_space[0]\n",
    "        regressor.znorm_space_bardist_ = znorm_space[0]\n",
    "        regressor.fit_from_preprocessed(X_tr, y_tr, cat_ixs, confs)\n",
    "        preds, _, _ = regressor.forward(X_te)\n",
    "        loss_fn = znorm_space[0]\n",
    "        loss = loss_fn(preds, y_te.to(regressor.device)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# --- Evaluation ---\n",
    "eval_reg = clone_model_for_evaluation(regressor, {}, TabPFNRegressor)\n",
    "eval_reg.fit(X_train, y_train)\n",
    "preds = eval_reg.predict(X_test)\n",
    "\n",
    "print(\"RMSE:\", rmse(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "fine-tuning indeed improves the performance\n",
    "\n",
    "---\n",
    "\n",
    "Tips\n",
    "\n",
    "1. using MB results to help regression task\n",
    "2. instead of fine-tuning datasets separately, you can fine-tune on all datasets together to improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Predicting MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "In the following, we describe a simple approach to estimate MB using TabPFN.\n",
    "\n",
    "A key characteristic of MB is that it remains the same for both the training and testing sets.\n",
    "Therefore, unlike standard target prediction tasks, we train TabPFN on MB from the training datasets \n",
    "and evaluate it on MB from the testing datasets.\n",
    "\n",
    "However, TabPFN natively supports only single-target prediction, so we need to adapt it for multi-output classification to predict the dimensions of MB.\n",
    "\n",
    "Another challenge is that different datasets have different MB dimensions.\n",
    "A simple solution is to train separate models for each MB dimension.\n",
    "In our case, there are only two MB sizes (9 and 19), so we train two independent models.\n",
    "\n",
    "Alternatively, one could pad MB vectors to a uniform size and train a single shared model across datasets.\n",
    "\n",
    "---\n",
    "\n",
    "Implementation details\n",
    "\n",
    "For each MB size, we train a model that maps  \n",
    "(n_dataset, n_estimators, n_samples, embedding_dim) -> (n_dataset, mb_dim)\n",
    "\n",
    "To achieve this, we proceed as follows:\n",
    "\n",
    "1. Compute embeddings of $X$ for each dataset using `nfold` to obtain more robust embeddings (https://arxiv.org/pdf/2502.17361).  \n",
    "   The resulting embeddings have shape (n_estimators * n_samples * embedding_dim = 192).\n",
    "2. Aggregate and reshape embeddings across estimators: \n",
    "   (n_dataset, n_estimators, n_samples, embedding_dim) -> (n_dataset * n_samples, embedding_dim).\n",
    "3. Expand MB labels accordingly:  \n",
    "   (n_dataset, mb_dim) -> (n_dataset * n_samples, mb_dim).\n",
    "4. Train a multi-output model (e.g., MultiOutputClassifier) on these processed embeddings.\n",
    "5. On the testing datasets, extract embeddings of $X$ and repeat step 2 for consistency.\n",
    "6. Predict MB for all samples, obtaining outputs of shape (n_dataset * n_samples, mb_dim).\n",
    "7. Aggregate predictions across samples within each dataset using a majority vote (or averaging with a 0.5 threshold) to recover dataset-level MB predictions:  \n",
    "   (n_dataset * n_samples, mb_dim) -> (n_dataset, mb_dim).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter mb size == 9\n",
    "# for the purpose of demo, we only use a subset of data with mb size 9\n",
    "data_mb9 = [d for d in develop if d['n_features'] == 9]\n",
    "train_data_mb9 = data_mb9[:20]  # use a smaller subset for faster demo\n",
    "test_data_mb9 = data_mb9[:10]  # use a smaller subset for faster demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TabPFN model\n",
    "regressor_config = {\n",
    "        \"ignore_pretraining_limits\": True,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"n_estimators\": 24,\n",
    "        \"random_state\":42,\n",
    "        \"inference_precision\": \"auto\"\n",
    "    }\n",
    "\n",
    "# --- Setup model ---\n",
    "regressor = TabPFNRegressor(\n",
    "    **regressor_config,\n",
    ")\n",
    "\n",
    "# create an embedding extractor\n",
    "embedding_extractor = TabPFNEmbedding(tabpfn_reg=regressor, n_fold=5)\n",
    "\n",
    "train_embeddings = []\n",
    "for d in tqdm(train_data_mb9):\n",
    "    train_embedding = embedding_extractor.get_embeddings(np.asarray(d['X_train']), np.asarray(d['y_train']), np.asarray(d['X_test']), data_source=\"train\")\n",
    "    train_embeddings.append(train_embedding)\n",
    "\n",
    "test_embeddings = []\n",
    "for d in tqdm(test_data_mb9):\n",
    "    test_embedding = embedding_extractor.get_embeddings(np.asarray(d['X_train']), np.asarray(d['y_train']), np.asarray(d['X_test']), data_source=\"test\")\n",
    "    test_embeddings.append(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(train_embeddings).shape # n_dataset * n_estimators * n_samples * embedding_dim (192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggreagte embeddings by n_estimators\n",
    "agg_embeddings = np.mean(np.stack(train_embeddings), axis=1).reshape(-1, 192)\n",
    "agg_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_train = np.stack([np.tile(d['feature_mask'], (d['n_train'], 1)) for d in train_data_mb9]).reshape(-1, 9)\n",
    "mb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiOutputClassifier(LogisticRegression(), n_jobs=4).fit(agg_embeddings, mb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict, switch case\n",
    "test_embeddings_agg = np.mean(np.stack(test_embeddings), axis=1).reshape(-1, 192)\n",
    "test_embeddings_agg.shape\n",
    "\n",
    "predicted_mb = clf.predict(test_embeddings_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mb_new = predicted_mb.reshape(len(test_data_mb9), -1, 9)\n",
    "for d in range(len(test_data_mb9)):\n",
    "    pred_mb = np.mean(pred_mb_new[d, :], axis=0) >= 0.5\n",
    "    pred_mb = pred_mb.astype(int)\n",
    "    true_mb = np.asarray(test_data_mb9[d]['feature_mask'])\n",
    "\n",
    "    print(f\"True MB: {true_mb}\")\n",
    "    print(f\"Predicted MB: {pred_mb}\")\n",
    "    print(\"Jaccard Score: \", jaccard_score(true_mb, pred_mb))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Tips\n",
    "\n",
    "1. MultiOutputClassifer treat each target independently, which is not ideal since mb features are correlated.\n",
    "2. Use a NN model to better predict mb from embedding (e.g. MLP, GNN, seq2seq etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "On submission dataset, for each dataset (data_id), predict both `y` and `markov_blanket`, and save results in `submission.csv` with the following format:\n",
    "\n",
    "| data_id | y_pred | markov_blanket_pred |\n",
    "|---------|--------|---------------------|\n",
    "| int     | float  | list of int         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For each dataset $j$ in `submit` with testing set $\\mathcal{D}^{(j)}_{\\text{test}}$ we compute:\n",
    "\n",
    "- **RMSE** on regression:\n",
    "$$ \\text{RMSE}_j = \\sqrt{\\frac{1}{N_{\\text{test}}} \\sum_{(x, y) \\in \\mathcal{D}^{(j)}_{\\text{test}}} (y - \\hat{y}(x))^2 }. $$\n",
    "- **Jaccard score** on MB masks:\n",
    "$$ \\text{Jaccard}_j = \\frac{|\\hat{m}^{(j)} \\cap m^{(j)}_{\\text{true}}|}{|\\hat{m}^{(j)} \\cup m^{(j)}_{\\text{true}}|}. $$\n",
    "\n",
    "Averaging over $N$ tasks gives $\\overline{\\text{RMSE}}$ and $\\overline{\\text{Jaccard}}$, and\n",
    "the final challenge **score** is\n",
    "$$ \\text{Score} = avg(RMSE_i * (1 - Jaccard_i)). $$\n",
    "\n",
    "- On the **develop test**, you can compute this score yourself.\n",
    "- On the **submission test**, the organizers run the same computation on a\n",
    "  hidden set of tasks.\n",
    "- Ground truth data will be released after the challenge ends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_task(\n",
    "    y_query_true: np.ndarray,\n",
    "    y_query_pred: np.ndarray,\n",
    "    mb_true: np.ndarray,\n",
    "    mb_pred: np.ndarray,\n",
    ") -> dict:\n",
    "    rmse_val = rmse(y_query_true, y_query_pred)\n",
    "    jaccard_val = jaccard_score(mb_true, mb_pred)\n",
    "    score_val = rmse_val * (1.0 - jaccard_val)\n",
    "    return {\"rmse\": rmse_val, \"jaccard\": jaccard_val, \"score\": score_val}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emu0atonqqv",
   "source": "# Complete Solution: MB-First Pipeline\n\nStrategy: Predict MB masks first using embeddings, then use filtered features for regression.\n\nArchitecture:\n1. Extract TabPFN embeddings for each dataset\n2. Train MLP classifiers to predict MB masks (separate models for 9-feat and 19-feat)\n3. Use predicted MB to filter features for TabPFN regression\n4. Generate predictions for submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gsx8sf6ftk8",
   "source": "# Split by n_features\ndata_9 = [d for d in develop if d['n_features'] == 9]\ndata_19 = [d for d in develop if d['n_features'] == 19]\n\nprint(f\"9-feature tasks: {len(data_9)}\")\nprint(f\"19-feature tasks: {len(data_19)}\")\n\n# Train/val split (80/20)\ntrain_9, val_9 = data_9[:70], data_9[70:]\ntrain_19, val_19 = data_19[:76], data_19[76:]\n\nprint(f\"\\nTrain/Val split:\")\nprint(f\"  9-feat:  train={len(train_9)}, val={len(val_9)}\")\nprint(f\"  19-feat: train={len(train_19)}, val={len(val_19)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "65mnxkepu2p",
   "source": "def extract_embeddings_for_dataset(dataset_list, n_fold=5, device='cuda'):\n    \"\"\"Extract TabPFN embeddings for a list of datasets.\"\"\"\n    regressor_config = {\n        \"ignore_pretraining_limits\": True,\n        \"device\": device,\n        \"n_estimators\": 8,  # Reduced for speed\n        \"random_state\": 42,\n        \"inference_precision\": \"auto\"\n    }\n    \n    regressor = TabPFNRegressor(model_path=model_path, **regressor_config)\n    embedding_extractor = TabPFNEmbedding(tabpfn_reg=regressor, n_fold=n_fold)\n    \n    all_embeddings = []\n    all_mb_masks = []\n    \n    print(f\"Extracting embeddings for {len(dataset_list)} datasets...\")\n    for d in tqdm(dataset_list, desc=\"Embedding extraction\"):\n        X_train = np.asarray(d['X_train'])\n        y_train = np.asarray(d['y_train'])\n        X_test = np.asarray(d['X_test'])\n        \n        # Get embeddings for train data\n        emb = embedding_extractor.get_embeddings(X_train, y_train, X_test, data_source=\"train\")\n        \n        # Aggregate across estimators (mean)\n        emb_agg = np.mean(emb, axis=0)  # (n_samples, embed_dim)\n        \n        all_embeddings.append(emb_agg)\n        all_mb_masks.append(np.asarray(d['feature_mask']))\n    \n    return all_embeddings, all_mb_masks\n\n# Extract for both 9-feat and 19-feat\nprint(\"\\\\n=== Extracting embeddings for 9-feature datasets ===\")\ntrain_9_emb, train_9_mb = extract_embeddings_for_dataset(train_9, device='cuda')\n\nprint(\"\\\\n=== Extracting embeddings for 19-feature datasets ===\")\ntrain_19_emb, train_19_mb = extract_embeddings_for_dataset(train_19, device='cuda')\n\nprint(\"\\\\nEmbedding extraction complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xhma98c743",
   "source": "import torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nclass MBPredictor(nn.Module):\n    def __init__(self, embed_dim=192, hidden_dims=[256, 128], n_features=9, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dims[0]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dims[0], hidden_dims[1]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dims[1], n_features),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef prepare_mb_training_data(embeddings_list, mb_masks_list, n_features):\n    \"\"\"Prepare training data for MB predictor.\n    \n    Args:\n        embeddings_list: List of (n_samples, embed_dim) arrays\n        mb_masks_list: List of (n_features,) binary masks\n        n_features: Number of features (9 or 19)\n    \n    Returns:\n        X: (n_datasets * n_samples, embed_dim) tensor\n        y: (n_datasets * n_samples, n_features) tensor\n    \"\"\"\n    X_all = []\n    y_all = []\n    \n    for emb, mb_mask in zip(embeddings_list, mb_masks_list):\n        n_samples = emb.shape[0]\n        # Replicate MB mask for each sample in the dataset\n        mb_replicated = np.tile(mb_mask, (n_samples, 1))\n        \n        X_all.append(emb)\n        y_all.append(mb_replicated)\n    \n    X = torch.FloatTensor(np.vstack(X_all))\n    y = torch.FloatTensor(np.vstack(y_all))\n    \n    return X, y\n\n# Prepare training data\nprint(\"Preparing training data for MB predictors...\")\nX_train_9, y_train_9 = prepare_mb_training_data(train_9_emb, train_9_mb, 9)\nX_train_19, y_train_19 = prepare_mb_training_data(train_19_emb, train_19_mb, 19)\n\nprint(f\"9-feat training data: X={X_train_9.shape}, y={y_train_9.shape}\")\nprint(f\"19-feat training data: X={X_train_19.shape}, y={y_train_19.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6hp6nvgacxn",
   "source": "def train_mb_predictor(X_train, y_train, n_features, epochs=100, batch_size=128, lr=0.001, device='cuda'):\n    \"\"\"Train MB predictor model.\"\"\"\n    model = MBPredictor(embed_dim=192, hidden_dims=[256, 128], n_features=n_features, dropout=0.3).to(device)\n    \n    # Calculate pos_weight for class imbalance\n    pos_weight = (y_train == 0).sum() / (y_train ==1).sum()\n    pos_weight = torch.FloatTensor([pos_weight]).to(device)\n    \n    criterion = nn.BCELoss()  # Binary cross-entropy\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    \n    # Create dataloader\n    dataset = TensorDataset(X_train, y_train)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    model.train()\n    best_loss = float('inf')\n    \n    for epoch in tqdm(range(epochs), desc=f\"Training MB-{n_features}\"):\n        total_loss = 0\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            optimizer.zero_grad()\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(loader)\n        if (epoch + 1) % 20 == 0:\n            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n    \n    return model\n\n# Train models\nprint(\"\\\\n=== Training 9-feature MB Predictor ===\")\nmb_model_9 = train_mb_predictor(X_train_9, y_train_9, n_features=9, epochs=100, device='cuda')\n\nprint(\"\\\\n=== Training 19-feature MB Predictor ===\")\nmb_model_19 = train_mb_predictor(X_train_19, y_train_19, n_features=19, epochs=100, device='cuda')\n\nprint(\"\\\\nTraining complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "no3ynkib8j",
   "source": "def predict_task(task_data, mb_model_9, mb_model_19, device='cuda'):\n    \"\"\"Complete pipeline: predict MB and y for a single task.\n    \n    Args:\n        task_data: Dict with 'X_train', 'y_train', 'X_test', 'n_features'\n        mb_model_9, mb_model_19: Trained MB predictor models\n        device: 'cuda' or 'cpu'\n    \n    Returns:\n        y_pred: (n_test,) array of predictions\n        mb_pred: (n_features,) binary array of predicted MB mask\n    \"\"\"\n    X_train = np.asarray(task_data['X_train'])\n    y_train = np.asarray(task_data['y_train'])\n    X_test = np.asarray(task_data['X_test'])\n    n_features = task_data['n_features']\n    \n    # Step 1: Extract embeddings\n    regressor_config = {\n        \"ignore_pretraining_limits\": True,\n        \"device\": device,\n        \"n_estimators\": 8,\n        \"random_state\": 42,\n        \"inference_precision\": \"auto\"\n    }\n    \n    regressor = TabPFNRegressor(model_path=model_path, **regressor_config)\n    embedding_extractor = TabPFNEmbedding(tabpfn_reg=regressor, n_fold=5)\n    \n    # Get embeddings (use combined train+test)\n    X_all = np.vstack([X_train, X_test])\n    y_all_temp = np.hstack([y_train, np.zeros(len(X_test))])  # Dummy y for test\n    embeddings = embedding_extractor.get_embeddings(X_train, y_train, X_test, data_source=\"train\")\n    \n    # Aggregate across estimators\n    emb_agg = np.mean(embeddings, axis=0)  # (n_samples, 192)\n    \n    # Step 2: Predict MB mask\n    mb_model = mb_model_9 if n_features == 9 else mb_model_19\n    mb_model.eval()\n    \n    with torch.no_grad():\n        X_emb = torch.FloatTensor(emb_agg).to(device)\n        mb_probs = mb_model(X_emb)  # (n_samples, n_features)\n        \n        # Aggregate predictions across samples (majority vote via mean > 0.5)\n        mb_pred = (mb_probs.mean(dim=0) > 0.5).int().cpu().numpy()\n    \n    # Ensure at least one feature is selected\n    if mb_pred.sum() == 0:\n        # Fallback: select top 3 features by probability\n        top_k = min(3, n_features)\n        top_indices = mb_probs.mean(dim=0).argsort(descending=True)[:top_k].cpu().numpy()\n        mb_pred[top_indices] = 1\n    \n    # Step 3: Filter features and run regression\n    X_train_filt = X_train[:, mb_pred == 1]\n    X_test_filt = X_test[:, mb_pred == 1]\n    \n    # TabPFN regression\n    regressor_final = TabPFNRegressor(\n        model_path=model_path,\n        device=device,\n        n_estimators=24,\n        ignore_pretraining_limits=True,\n        random_state=42\n    )\n    \n    regressor_final.fit(X_train_filt, y_train)\n    y_pred = regressor_final.predict(X_test_filt)\n    \n    return y_pred, mb_pred\n\nprint(\"Prediction pipeline defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "o0famfhik9",
   "source": "# Validate on validation sets\nval_results = []\n\nprint(\"\\\\n=== Validating on 9-feature tasks ===\")\nfor task in tqdm(val_9[:5], desc=\"Val 9-feat\"):  # Sample first 5 for speed\n    y_pred, mb_pred = predict_task(task, mb_model_9, mb_model_19, device='cuda')\n    \n    y_true = np.asarray(task['y_test'])\n    mb_true = np.asarray(task['feature_mask'])\n    \n    # Compute metrics\n    task_rmse = rmse(y_true, y_pred)\n    task_jaccard = jaccard_score(mb_true, mb_pred)\n    task_score = task_rmse * (1.0 - task_jaccard)\n    \n    val_results.append({\n        'n_features': 9,\n        'rmse': task_rmse,\n        'jaccard': task_jaccard,\n        'score': task_score\n    })\n\nprint(\"\\\\n=== Validating on 19-feature tasks ===\")\nfor task in tqdm(val_19[:5], desc=\"Val 19-feat\"):  # Sample first 5 for speed\n    y_pred, mb_pred = predict_task(task, mb_model_9, mb_model_19, device='cuda')\n    \n    y_true = np.asarray(task['y_test'])\n    mb_true = np.asarray(task['feature_mask'])\n    \n    task_rmse = rmse(y_true, y_pred)\n    task_jaccard = jaccard_score(mb_true, mb_pred)\n    task_score = task_rmse * (1.0 - task_jaccard)\n    \n    val_results.append({\n        'n_features': 19,\n        'rmse': task_rmse,\n        'jaccard': task_jaccard,\n        'score': task_score\n    })\n\n# Compute average metrics\nval_df = pd.DataFrame(val_results)\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"Average RMSE: {val_df['rmse'].mean():.4f}\")\nprint(f\"Average Jaccard: {val_df['jaccard'].mean():.4f}\")\nprint(f\"Average Score: {val_df['score'].mean():.4f} (lower is better)\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "379jzuwds3h",
   "source": "# Generate submission\nsubmission_results = []\n\nprint(\"\\\\n=== Generating Predictions for Submission ===\")\nprint(f\"Total submit tasks: {len(submit)}\")\n\nfor task in tqdm(submit, desc=\"Submission\"):\n    data_id = task['data_id']\n    \n    # Run prediction\n    y_pred, mb_pred = predict_task(task, mb_model_9, mb_model_19, device='cuda')\n    \n    submission_results.append({\n        'data_id': data_id,\n        'y_pred': y_pred.tolist(),  # Convert to list for CSV\n        'markov_blanket_pred': mb_pred.tolist()\n    })\n\n# Create submission dataframe\nsubmission_df = pd.DataFrame(submission_results)\n\n# Save to CSV\nsubmission_path = '/home/mtopiwal/CSE472-blanket-challenge/submission.csv'\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(f\"\\\\n\u2713 Submission saved to: {submission_path}\")\nprint(f\"\u2713 Total tasks: {len(submission_df)}\")\nprint(\"\\\\nFirst 3 submissions:\")\nprint(submission_df.head(3))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5pppy44efp",
   "source": "## Step 7: Generate Submission\n\nRun prediction pipeline on all 46 submit tasks and create submission.csv.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qqoj0sm47vh",
   "source": "## Step 6: Validation on Develop Set\n\nEvaluate performance on validation split to estimate final score.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "kxm915kqa9q",
   "source": "## Step 5: Complete Prediction Pipeline\n\nPredict MB \u2192 Filter features \u2192 Regress with TabPFN.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ppno2pw2u3j",
   "source": "## Step 4: Train MB Predictors\n\nTrain separate MLP models for 9-feature and 19-feature tasks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "svu2fi3l4pr",
   "source": "## Step 3: Define MLP MB Predictor\n\nMulti-layer perceptron for predicting binary MB masks from embeddings.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "883xod2g41a",
   "source": "## Step 2: Extract TabPFN Embeddings\n\nExtract embeddings from training data for MB prediction.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ochdktntcaq",
   "source": "## Step 1: Data Preparation\n\nSplit develop set by n_features and create train/val splits.",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}